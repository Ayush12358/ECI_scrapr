{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying everything again\n",
    "using xpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a categorized list of all the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "from lxml import html\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "import time\n",
    "from http.client import RemoteDisconnected\n",
    "from urllib3.exceptions import MaxRetryError\n",
    "from requests.exceptions import Timeout\n",
    "\n",
    "d_process = 0\n",
    "retries = 3\n",
    "delay = 5\n",
    "base_url = \"https://eci.gov.in\"\n",
    "# use this\n",
    "def get_json():\n",
    "    lists = {} # dictionary to store all the links\n",
    "    for i in range(0,4):\n",
    "        time.sleep(delay)\n",
    "        linkss = {}\n",
    "        if(i==0):\n",
    "            \n",
    "            response = requests.get(\"https://eci.gov.in/statistical-report/statistical-reports/\")\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            body = soup.find(id=\"elCmsPageWrap\").select ('table')[i].select('em')[0].text.strip()\n",
    "            # print (body)\n",
    "            for link in soup.find(id=\"elCmsPageWrap\").select ('table')[i].select('a'):\n",
    "                name = link.text.strip()\n",
    "                link = link.get('href')\n",
    "                linkss[name] = link\n",
    "        if(i==1):\n",
    "            response = requests.get(\"https://eci.gov.in/statistical-report/statistical-reports/\")\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            body = soup.find(id=\"elCmsPageWrap\").select ('table')[1].select('em')[0].text.strip()\n",
    "            states = {}\n",
    "            state_soup = soup.find(id=\"elCmsPageWrap\").select ('table')[1].select('tr')\n",
    "            state_soup = state_soup[1:]\n",
    "            for one in state_soup:\n",
    "                two = one.select('td')[0]\n",
    "                one = one.select('td')[1]\n",
    "                link_a = one.select('a') \n",
    "                link_name = {}\n",
    "                # print (two.text.strip())\n",
    "                for linka in link_a:\n",
    "                    name = linka.text.strip()\n",
    "                    link = linka.get('href')\n",
    "                    # print ('\\t',name, link)\n",
    "                    link_name[name] = link\n",
    "                states[two.text.strip()] = link_name\n",
    "            linkss = states\n",
    "        #     print (linkss)\n",
    "        # else: continue ## to be removed\n",
    "        if(i==2):\n",
    "            response = requests.get(\"https://eci.gov.in/statistical-report/statistical-reports/\")\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            body = soup.find(id=\"elCmsPageWrap\").select ('table')[i].select('em')[0].text.strip()\n",
    "            soup = soup.find(id=\"elCmsPageWrap\").select ('table')[i].select('tr')\n",
    "            states = {}\n",
    "            for i in range(1,len(soup)):\n",
    "                links = {}\n",
    "                state = soup[i].select('td')[0].text.strip()\n",
    "                for link in soup[i].select('td')[1].select('a'):\n",
    "                    name = link.text.strip()\n",
    "                    link = link.get('href')\n",
    "                    links[name] = link\n",
    "                # print (state, links)\n",
    "                states[state] = links\n",
    "                state = soup[i].select('td')[2].text.strip()\n",
    "                for link in soup[i].select('td')[3].select('a'):\n",
    "                    name = link.text.strip()\n",
    "                    link = link.get('href')\n",
    "                    links[name] = link\n",
    "                # print (state, links)\n",
    "                states[state] = links\n",
    "            linkss = states\n",
    "        if(i==3):\n",
    "            response = requests.get(\"https://eci.gov.in/statistical-report/statistical-reports/\")\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            body = soup.find(id=\"elCmsPageWrap\").select ('table')[i].select('em')[0].text.strip()\n",
    "            soup = soup.find(id=\"elCmsPageWrap\").select ('table')[3].select('tr')\n",
    "            category = {}\n",
    "            new_soup = soup\n",
    "            links = {}\n",
    "            names = []\n",
    "            link = []\n",
    "            for k in range (1,len(new_soup)-1):\n",
    "                name = new_soup[k].select('td')[0].select('a')\n",
    "                for j in name:\n",
    "                    names.append(j.text.strip())\n",
    "                    link.append(j.get('href'))\n",
    "            for k in range (0,len(names)):\n",
    "                links[names[k]] = link[k]\n",
    "            category['GENERAL ELECTION TO STATE LEGISLATIVE ASSEMBLY'] = links\n",
    "            links = {}\n",
    "            names = []\n",
    "            link = []\n",
    "            for k in range (1,len(new_soup)-1):\n",
    "                name = new_soup[k].select('td')[1].select('a')\n",
    "                for j in name:\n",
    "                    names.append(j.text.strip())\n",
    "                    link.append(j.get('href'))\n",
    "            for k in range (0,len(names)):\n",
    "                links[names[k]] = link[k]\n",
    "            category['GENERAL ELECTION TO LOK SABHA'] = links\n",
    "            linkss = category\n",
    "        lists[body] = linkss\n",
    "    # write to json file\n",
    "    with open('links.json', 'w') as f:\n",
    "        json.dump(lists, f, indent=4)\n",
    "    return lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_url(url):\n",
    "    if base_url not in url:\n",
    "        url = base_url+url\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "        except (requests.exceptions.RequestException, ConnectionError, OSError, ConnectionAbortedError, Timeout, RemoteDisconnected, MaxRetryError)as e:\n",
    "            # print(\"Connection er: \",e)\n",
    "            # time.sleep(delay)\n",
    "            print (\"Retrying...\")\n",
    "    print (\"Failed after \",retries,\" retries\")\n",
    "    return None\n",
    "\n",
    "def req(xpath,u): # pass a single link\n",
    "    if u == None:\n",
    "        return None\n",
    "    if base_url not in u:\n",
    "        u = base_url+u\n",
    "    \n",
    "    response = requests.get(u) ###\n",
    "    response = fetch_url(u)\n",
    "    if response!=None:\n",
    "        tree = html.fromstring(response.content)\n",
    "        components = tree.xpath(xpath)\n",
    "        if components:\n",
    "            return components[0].get('href')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def link_to_down(link): # pass a single link\n",
    "    if type (link) == list and len(link) == 1:\n",
    "        return link\n",
    "    elif type (link) == list and len(link) > 1:\n",
    "        link = link[0]\n",
    "    if link == None:\n",
    "        return None\n",
    "    if base_url not in link:\n",
    "        link = base_url+link\n",
    "    link_list = []\n",
    "    xpath = '/html/body/main/div[3]/div/div/div/div[2]/div[1]/aside/div/ul[1]/li/a' # download this file\n",
    "    links_req = link\n",
    "    link_req = req(xpath,links_req) # download this file\n",
    "    if link_req!=None:\n",
    "        links_req = link_req\n",
    "    xpath = '/html/body/main/div[3]/div/div/div/div[2]/ul/li[2]/a' # agree & download\n",
    "    link_req = req(xpath,links_req) # agree & download\n",
    "    i=1\n",
    "    if link_req!=None:\n",
    "        links_req = link_req\n",
    "    xpath = f'/html/body/main/div[3]/div/div/div/div/ul/li[{i}]/div[2]/a'\n",
    "    link_req = req(xpath,links_req) # download\n",
    "    if link_req==None:\n",
    "        return None\n",
    "    while True:\n",
    "        xpath = f'/html/body/main/div[3]/div/div/div/div/ul/li[{i}]/div[2]/a' # download\n",
    "        link_req = req(xpath,links_req)\n",
    "        if link_req==None:\n",
    "            break\n",
    "        link_list.append(link_req)\n",
    "        i+=1\n",
    "    return link_list\n",
    "\n",
    "def fix(input_string):\n",
    "    # Replace symbols (excluding . and /) with underscores\n",
    "    fixed_string = re.sub(r'[^a-zA-Z0-9]', '', input_string)\n",
    "    # Replace double backslashes with a single forward slash\n",
    "    fixed_string = fixed_string.replace('\\\\', '/')\n",
    "    # Alternatively, you can replace double backslashes with a double backslash\n",
    "    # fixed_string = fixed_string.replace('\\\\\\\\', '\\\\\\\\')\n",
    "    \n",
    "    return fixed_string\n",
    "\n",
    "def fix_file_name(input_string):\n",
    "    # Replace symbols (excluding . and /) with underscores\n",
    "    fixed_string = re.sub(r'[^a-zA-Z0-9.]', '', input_string)\n",
    "    # Replace double backslashes with a single forward slash\n",
    "    fixed_string = fixed_string.replace('\\\\', '/')\n",
    "    # Alternatively, you can replace double backslashes with a double backslash\n",
    "    # fixed_string = fixed_string.replace('\\\\\\\\', '\\\\\\\\')\n",
    "    \n",
    "    return fixed_string\n",
    "\n",
    "# use this\n",
    "def get_file_name_from_url(url):\n",
    "    \n",
    "    response = requests.head(url)\n",
    "    content_disposition = response.headers.get(\"content-disposition\")\n",
    "    if content_disposition and \"filename=\" in content_disposition:\n",
    "        file_name = unquote(content_disposition.split(\"filename=\")[1])\n",
    "    else:\n",
    "        file_name = url.split(\"/\")[-2]\n",
    "        file_name = file_name + \".pdf\"\n",
    "    file_name = fix_file_name(file_name)\n",
    "    return file_name\n",
    "\n",
    "def download_file(urls,directory):\n",
    "    \n",
    "    urls = link_to_down(urls)\n",
    "    if urls == None:\n",
    "        print (\"No file\", urls)\n",
    "        return\n",
    "    for url in urls:\n",
    "        if url == None:\n",
    "            print (\"No file\", urls)\n",
    "            continue\n",
    "        filename = get_file_name_from_url(url)\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        file_path = file_path.replace('\\\\', '/')\n",
    "        # file_path = os.path.join(file_path, \"file.txt\")\n",
    "        i=0\n",
    "        while(i<retries):\n",
    "            i+=1\n",
    "            try:\n",
    "                # check if file_path exists\n",
    "                if not os.path.exists(os.path.dirname(file_path)):\n",
    "                    # build file_path or file path\n",
    "                    response = requests.get(url)\n",
    "                    os.makedirs(os.path.dirname(file_path))\n",
    "                else: break\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                break\n",
    "            except (requests.exceptions.RequestException, ConnectionError, OSError, ConnectionAbortedError, Timeout, RemoteDisconnected, MaxRetryError)as e:\n",
    "                # print(\"Connection er: \",e)\n",
    "                # time.sleep(delay)\n",
    "                print (\"Retrying Download...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding within categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# new\n",
    "def broaden_save(link, directory):\n",
    "    if type(link)==type([]):\n",
    "        print (\"Error: pass a single link\")\n",
    "        print (link)\n",
    "        return\n",
    "    links = link\n",
    "    if links == None:\n",
    "        return\n",
    "    if base_url not in links:\n",
    "        links = base_url+links\n",
    "    list_links = []\n",
    "    skip = []\n",
    "    if type(links)==type([]):\n",
    "        if len(links)==1:\n",
    "            links =links[0]\n",
    "    temp_link=link_to_down(links)\n",
    "    if temp_link!=None or temp_link!=[]:\n",
    "        download_file(temp_link, directory)\n",
    "    else: # navigate through the paginated list\n",
    "        # first page\n",
    "        for j in range(1,26):\n",
    "            list_link = req(f'/html/body/main/div[3]/div/div[1]/div[2]/div/div/ol/li[{j}]/div[1]/a',links)\n",
    "            if list_link==None:\n",
    "                skip.append(j)\n",
    "                continue\n",
    "            temp_link = list_link\n",
    "            if temp_link!=None:\n",
    "                list_links.append(temp_link)\n",
    "        # other pages\n",
    "        cur_page = links\n",
    "        next_page = req('/html/body/main/div[3]/div/div[1]/div[2]/div/div/div[1]/div/ul/li[5]/a',cur_page)\n",
    "        while (next_page!=cur_page):\n",
    "            cur_page = next_page\n",
    "            next_page = req('/html/body/main/div[3]/div/div[1]/div[2]/div/div/div[1]/div/ul/li[5]/a',cur_page)\n",
    "            # next pages\n",
    "            for j in range(1,26):\n",
    "                list_link = req(f'/html/body/main/div[3]/div/div[1]/div[2]/div/div/ol/li[{j}]/div[2]/h4/span/a',cur_page)\n",
    "                if list_link==None:\n",
    "                    skip.append(j)\n",
    "                    continue\n",
    "                temp_link = list_link\n",
    "                if temp_link!=None:\n",
    "                    list_links.append(temp_link)\n",
    "        # all list link added to list_links\n",
    "    if len(skip)==50:\n",
    "        download_file(links, directory)\n",
    "    for link in list_links:\n",
    "        download_file(link, directory)\n",
    "\n",
    "def expand_save(directory, dict):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    for key in dict.keys():\n",
    "        if type(dict[key])==type({}):\n",
    "            expand_save(directory+'/'+fix(key),dict[key])\n",
    "        elif type(dict[key])==type([]):\n",
    "            for link in dict[key]:\n",
    "                broaden_save(link, directory+'/'+fix(key))\n",
    "        elif dict[key]==None:\n",
    "            continue\n",
    "        else:\n",
    "            broaden_save(dict[key], directory+'/'+fix(key))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = get_json()\n",
    "lists[\"Miscellaneous\"] = \"https://eci.gov.in/files/category/968-miscellaneous-statics/\"\n",
    "lists[\"General Election 2014\"] = \"https://eci.gov.in/files/category/97-general-election-2014/\"\n",
    "lists[\"General Election 2019 (Excluding Vellore PC)\"] = \"https://eci.gov.in/files/category/1359-general-election-2019-excluding-vellore-pc/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lists to json file\n",
    "with open('links.json', 'w') as f:\n",
    "    json.dump(lists, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary from json file\n",
    "# file is links.json\n",
    "with open('links.json') as f:\n",
    "    lists = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scrapping has not been done for the following pages:\n",
    "- https://eci.gov.in/statistical-report/link-to-form-20/\n",
    "- https://eci.gov.in/statistical-report/detailed-bye-election-results/\n",
    "- https://eci.gov.in/bye-election/bye-election-result/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lists = expand_save('./ECI_data',lists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
